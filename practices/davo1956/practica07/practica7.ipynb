{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practica 07"
      ],
      "metadata": {
        "id": "LpkmLfvYT4uF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alumno: David Pérez Jacome \\\\\n",
        "Número de Cuenta: 316330420"
      ],
      "metadata": {
        "id": "lpllz6GNT9KF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actividades**\n",
        "\n",
        "1. Explorar los datasets disponibles en el Shared Task de Open Machine Translation de AmericasNLP 2021\n",
        "  - Datasets\n",
        "  - Readme\n",
        "\n",
        "2. Crear un modelo de traducción neuronal usando OpenNMT-py y siguiendo el pipeline visto en clase\n",
        "  - 0.Obtención de datos y preprocesamiento\n",
        "    - Considerar que tiene que entrenar su modelo de tokenization\n",
        "  - a.Configuración y entrenamiento del modelo\n",
        "  - b.Traducción\n",
        "  - c.Evaluación\n",
        "    - Reportar BLEU\n",
        "    - Reportar ChrF (medida propuesta para el shared task)\n",
        "3. Comparar resultados con baseline\n",
        "4. Incluir el archivo *.translated.desubword\n",
        "\n",
        "**Extra**\n",
        "\n",
        "1. Investigar porque se propuso la medida ChrF en el Shared Task\n",
        "  - ¿Como se diferencia de BLEU?\n",
        "  - ¿Porqué es reelevante utilizar otras medidas de evaluación además de BLEU?\n"
      ],
      "metadata": {
        "id": "_xfRiiyDUALH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El propósito de la medida ChrF (Character F-score) en el contexto del Shared Task podría estar relacionado con las limitaciones percibidas del BLEU (Bilingual Evaluation Understudy) y la necesidad de medidas de evaluación más robustas y adecuadas para ciertos tipos de traducción.\n",
        "\n",
        "1. Enfoque en caracteres: ChrF se basa en la coincidencia de caracteres, lo que lo hace adecuado para lenguajes que pueden tener una gran variabilidad léxica o una riqueza morfológica, como los idiomas aglutinantes o los lenguajes con alfabetos no latinos.\n",
        "\n",
        "2. Sensibilidad a la fluidez: A diferencia de BLEU, que se centra en la coincidencia de n-gramas, ChrF también tiene en cuenta la fluidez de la traducción al considerar la cantidad de palabras y caracteres comunes entre las traducciones de referencia y la hipótesis de traducción.\n",
        "\n",
        "3. Adaptabilidad a diferentes tareas y dominios: ChrF puede ser más flexible que BLEU para adaptarse a diferentes dominios o tareas de traducción, ya que su enfoque en caracteres puede capturar mejor las similitudes entre las traducciones de referencia y la salida del modelo en diversas condiciones.\n",
        "\n",
        "**Diferencias entre ChrF y BLEU:**\n",
        "\n",
        "1. Caracteres vs. n-gramas: ChrF se centra en la coincidencia de caracteres, mientras que BLEU se basa en la coincidencia de n-gramas (secuencias de palabras). Esto hace que ChrF sea más robusto para lenguajes con estructuras morfológicas complejas o para tareas donde la precisión de caracteres es crítica.\n",
        "2. Sensibilidad a la fluidez: ChrF tiene en cuenta tanto la precisión como la fluidez, mientras que BLEU se centra principalmente en la precisión. Esto significa que ChrF puede penalizar las traducciones que son gramaticalmente incorrectas o que tienen una estructura incoherente.\n",
        "\n",
        "**Importancia de utilizar otras medidas de evaluación además de BLEU**\n",
        "\n",
        "1. Limitaciones de BLEU: BLEU es una métrica popular pero tiene sus limitaciones, como la insensibilidad a la reordenación de palabras, la falta de sensibilidad a la fluidez y la incapacidad para capturar la semántica de las traducciones. Por lo tanto, utilizar solo BLEU puede no proporcionar una evaluación completa y precisa del rendimiento del modelo de traducción.\n",
        "2. Variedad de tareas y lenguajes: Diferentes tareas de traducción y lenguajes pueden requerir métricas de evaluación diferentes. Al utilizar otras medidas como ChrF, se puede obtener una evaluación más completa que tenga en cuenta la diversidad lingüística y las características específicas de la tarea."
      ],
      "metadata": {
        "id": "r_fW7NZoUI9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ymoslem/MT-Preparation.git\n",
        "!pip3 install -r MT-Preparation/requirements.txt\n",
        "\n",
        "!pip install OpenNMT-py -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ili5psc3UKUF",
        "outputId": "ef1a0792-0930-4562-d27c-425ec6592a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MT-Preparation' already exists and is not an empty directory.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 2)) (2.0.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 3)) (0.1.99)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r MT-Preparation/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: OpenNMT-py in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: torch<2.3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.2)\n",
            "Requirement already satisfied: configargparse in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.7)\n",
            "Requirement already satisfied: ctranslate2<5,>=4 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (4.2.1)\n",
            "Requirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.15.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n",
            "Requirement already satisfied: waitress in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.0.0)\n",
            "Requirement already satisfied: pyonmttok<2,>=1.37 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.37.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.1)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.4.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.9.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.1.0)\n",
            "Requirement already satisfied: fasttext-wheel in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (0.9.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.7.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (1.25.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.64.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3,>=2.1->OpenNMT-py) (12.5.40)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel->OpenNMT-py) (2.12.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (4.66.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3,>=2.1->OpenNMT-py) (2.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->OpenNMT-py) (0.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3,>=2.1->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->OpenNMT-py) (1.1.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Setup Google Drive storage\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ6RsYG1Vg_Z",
        "outputId": "b62ae751-b8be-4621-cf65-69ea4699aea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"---Cloning AmericasNLP repository---\"\n",
        "!git clone https://github.com/AmericasNLP/americasnlp2021.git\n",
        "\n",
        "!echo \"---Concatenating Spanish dev and train datasets---\"\n",
        "!cat americasnlp2021/data/nahuatl-spanish/dev.es americasnlp2021/data/nahuatl-spanish/train.es > MT-Preparation/americasnlp2021.es-nah.es\n",
        "\n",
        "!echo \"---Concatenating Nahuatl dev and train datasets---\"\n",
        "!cat americasnlp2021/data/nahuatl-spanish/dev.nah americasnlp2021/data/nahuatl-spanish/train.nah > MT-Preparation/americasnlp2021.nah-es.nah"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gopPm4JEVtQ8",
        "outputId": "76568ef8-842d-412a-d1d8-b0f77dfa638c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Cloning AmericasNLP repository---\n",
            "Cloning into 'americasnlp2021'...\n",
            "remote: Enumerating objects: 469, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 469 (delta 89), reused 99 (delta 87), pack-reused 333\u001b[K\n",
            "Receiving objects: 100% (469/469), 37.37 MiB | 14.83 MiB/s, done.\n",
            "Resolving deltas: 100% (218/218), done.\n",
            "Updating files: 100% (146/146), done.\n",
            "---Concatenating Spanish dev and train datasets---\n",
            "---Concatenating Nahuatl dev and train datasets---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Preparacion\n",
        "\"\"\"\n",
        "\n",
        "!echo \"---Counting lines in original Nahuatl dataset---\"\n",
        "!wc -l MT-Preparation/americasnlp2021.nah-es.nah\n",
        "\n",
        "!echo \"---Counting lines in original Spanish dataset---\"\n",
        "!wc -l MT-Preparation/americasnlp2021.es-nah.es\n",
        "\n",
        "!echo \"---Running filter.py script---\"\n",
        "!python3 MT-Preparation/filtering/filter.py MT-Preparation/americasnlp2021.nah-es.nah MT-Preparation/americasnlp2021.es-nah.es nah es\n",
        "\n",
        "!echo \"---Counting lines in filtered Nahuatl dataset---\"\n",
        "!wc -l MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah\n",
        "\n",
        "!echo \"---Counting lines in filtered Spanish dataset---\"\n",
        "!wc -l MT-Preparation/americasnlp2021.es-nah.es-filtered.es"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsnZmNo8Vz-a",
        "outputId": "6f8f194d-843a-4fe5-c9e9-750a503ac3bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Counting lines in original Nahuatl dataset---\n",
            "16817 MT-Preparation/americasnlp2021.nah-es.nah\n",
            "---Counting lines in original Spanish dataset---\n",
            "16817 MT-Preparation/americasnlp2021.es-nah.es\n",
            "---Running filter.py script---\n",
            "Dataframe shape (rows, columns): (16817, 2)\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 16733\n",
            "--- Duplicates Deleted\t\t\t--> Rows: 16119\n",
            "--- Source-Copied Rows Deleted\t\t--> Rows: 16044\n",
            "--- Too Long Source/Target Deleted\t--> Rows: 14783\n",
            "--- HTML Removed\t\t\t--> Rows: 14783\n",
            "--- Rows will remain in true-cased\t--> Rows: 14783\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 14783\n",
            "--- Rows Shuffled\t\t\t--> Rows: 14783\n",
            "--- Source Saved: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah\n",
            "--- Target Saved: MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n",
            "---Counting lines in filtered Nahuatl dataset---\n",
            "14783 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah\n",
            "---Counting lines in filtered Spanish dataset---\n",
            "14783 MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"---Listing files in MT-Preparation/subwording directory---\"\n",
        "!ls MT-Preparation/subwording/\n",
        "\n",
        "!echo \"---Running 1-train_unigram.py script---\"\n",
        "!python3 MT-Preparation/subwording/1-train_unigram.py MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah MT-Preparation/americasnlp2021.es-nah.es-filtered.es"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9y_BDYzV7ft",
        "outputId": "f433eec4-54f2-4f91-8494-19b0010ae42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Listing files in MT-Preparation/subwording directory---\n",
            "1-train_bpe.py\t1-train_unigram.py  2-subword.py  3-desubword.py\n",
            "---Running 1-train_unigram.py script---\n",
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah\n",
            "  input_format: \n",
            "  model_prefix: source\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 50000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 1\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 0\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 14783 sentences\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=2053564\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9528% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=87\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999528\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 14783 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=1131603\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 131964 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 14783\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 51930\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 51930 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=39106 obj=12.5167 num_tokens=110092 num_tokens/piece=2.81522\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=32625 obj=10.3851 num_tokens=111630 num_tokens/piece=3.42161\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: source.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: source.vocab\n",
            "Done, training a SentencepPiece model for the Source finished successfully!\n",
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=MT-Preparation/americasnlp2021.es-nah.es-filtered.es --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n",
            "  input_format: \n",
            "  model_prefix: target\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 50000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 1\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 0\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 14783 sentences\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=1982306\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9582% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=81\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999582\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 14783 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=1044105\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 55218 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 14783\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 26627\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 26627 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20221 obj=10.2914 num_tokens=53863 num_tokens/piece=2.66372\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16504 obj=8.50295 num_tokens=54464 num_tokens/piece=3.30005\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: target.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: target.vocab\n",
            "Done, training a SentencepPiece model for the Target finished successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"---Listing files in current directory---\"\n",
        "!ls\n",
        "\n",
        "!echo \"---Running 2-subword.py script---\"\n",
        "!python3 MT-Preparation/subwording/2-subword.py source.model target.model MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n",
        "\n",
        "!echo \"---First 3 lines of filtered Nahuatl and Spanish datasets---\"\n",
        "!head -n 3 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah && echo \"-----\" && head -n 3 MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n",
        "\n",
        "!echo \"---First 10 lines of subworded Nahuatl and Spanish datasets---\"\n",
        "!head -n 10 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword && echo \"---\" && head -n 10 MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOclSx5iWB88",
        "outputId": "03d127f4-2fab-411e-ff73-953256cce16e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Listing files in current directory---\n",
            "americasnlp2021  MT-Preparation  source.model  target.model\n",
            "drive\t\t sample_data\t source.vocab  target.vocab\n",
            "---Running 2-subword.py script---\n",
            "Source Model: source.model\n",
            "Target Model: target.model\n",
            "Source Dataset: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah\n",
            "Target Dataset: MT-Preparation/americasnlp2021.es-nah.es-filtered.es\n",
            "Done subwording the source file! Output: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword\n",
            "Done subwording the target file! Output: MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword\n",
            "---First 3 lines of filtered Nahuatl and Spanish datasets---\n",
            "Yoan nicteneua cempoaltetl omome xicalpechtotontin achi yaiancuic yntech pouiz yn noxhuiuan Joana Maximiliana yoan Diego de Castañeda yoan yn iteicauh Balthazar .\n",
            "Juebes yc XVII mani metztli de junio , yquac motlayahualhui in Sacramento , auh amo huel mochiuh amo huel mohtlatoquilti , çan callitic yn motlayahualhui yn iglesia mayor ; ynic ahuel moquixti ypampa cenca quiyauh , huel totocac in quiahuitl yhuan cenca çoquititlan catca chiuh .\n",
            "Ipa mochiua saj , miak mochiua .\n",
            "-----\n",
            "Yten declaro gue tengo veynte y dos xicaras casi nuevas mando las aya la dicha mi nieta Juana Maximiliana y Diego de Castañeda y su hermano [ menor ] Baltasar .\n",
            "El jueves 17 de junio se hizo la procesión del Santísimo Sacramento , pero no pudo hacerse bien ni salir , sino que sólo se hizo dentro de la iglesia mayor ; no pudo salir porque llovía mucho , cayó un fuerte aguacero y se formó mucho lodo .\n",
            "Se da solo . Se da mucho .\n",
            "---First 10 lines of subworded Nahuatl and Spanish datasets---\n",
            "▁Y oan ▁ni cteneua ▁cempoal tetl ▁omome ▁xical pe chtotonti n ▁achi ▁ya ia ncuic ▁yntech ▁pouiz ▁yn ▁noxhui uan ▁Joan a ▁Maximilian a ▁y oan ▁Diego ▁de ▁C astañ eda ▁y oan ▁yn ▁ite icauh ▁Baltha zar ▁.\n",
            "▁Ju ebes ▁yc ▁XVII ▁mani ▁metztli ▁de ▁juni o ▁, ▁y quac ▁motlayahualhui ▁in ▁Sacramen to ▁, ▁auh ▁amo ▁huel ▁mochiuh ▁amo ▁huel ▁ mohtlatoquil ti ▁, ▁çan ▁ callitic ▁yn ▁motlayahualhui ▁yn ▁i glesia ▁mayor ▁; ▁ynic ▁ahuel ▁moquixti ▁ypampa ▁cenca ▁quiyauh ▁, ▁huel ▁ totocac ▁in ▁quiahui tl ▁yhuan ▁cenca ▁çoqui titlan ▁catca ▁chiuh ▁.\n",
            "▁Ipa ▁mochiua ▁saj ▁, ▁miak ▁mochiua ▁.\n",
            "▁X ▁Tocht li ▁xihuitl ▁, ▁ 1 2 4 2 ▁años ▁.\n",
            "▁Ce ▁tonal tica ▁amo ▁molini que ▁nian ▁zapatista s ▁nian ▁carrancista s ▁. ▁Otec ahque ▁ma ▁motoca can ▁mimiqu ez ▁.\n",
            "▁ika ▁expa ▁kipanolti ▁ipa ▁metla tl ▁para\n",
            "▁Ipan ▁calohtli pa ▁hueyi ▁altepetl ▁nipaxialo htinemi ▁.\n",
            "▁Para ▁na ▁niuala\n",
            "▁X ▁Tocht li ▁xihuitl ▁, ▁ 1 3 4 6 ▁.\n",
            "▁ti quintayocoli ▁cuahcual i ▁.\n",
            "---\n",
            "▁Y ten ▁de claro ▁gue ▁teng o ▁ve ynte ▁y ▁dos ▁xica ras ▁casi ▁nuevas ▁mando ▁las ▁aya ▁la ▁dicha ▁mi ▁nieta ▁Juan a ▁Maximilian a ▁y ▁Dieg o ▁de ▁Cas tañed a ▁y ▁su ▁hermano ▁[ ▁menor ▁] ▁Balta sar ▁.\n",
            "▁El ▁jueve s ▁ 1 7 ▁de ▁jun io ▁se ▁hizo ▁la ▁procesi ón ▁del ▁Santísim o ▁S acramento ▁, ▁pero ▁no ▁pud o ▁hacer se ▁bien ▁ni ▁sali r ▁, ▁sin o ▁que ▁s ólo ▁se ▁hizo ▁dentro ▁de ▁la ▁iglesia ▁mayor ▁; ▁no ▁pud o ▁sali r ▁porque ▁llov ía ▁mucho ▁, ▁cayó ▁un ▁fuerte ▁aguacero ▁y ▁se ▁form ó ▁mucho ▁lo do ▁.\n",
            "▁Se ▁da ▁solo ▁. ▁Se ▁da ▁mucho ▁.\n",
            "▁ 1 0 ▁Tochtl i ▁, ▁ 1 2 4 2 ▁.\n",
            "▁D urante ▁todo ▁un ▁día ▁no ▁se ▁movier on ▁ni ▁los ▁carrancista s ▁ni ▁los ▁zapatista s ▁. ▁De jaron ▁que ▁se ▁enterrar a ▁a ▁los ▁muertos ▁.\n",
            "▁tres ▁veces ▁lo ▁pasó ▁sobre ▁el ▁metate ▁para\n",
            "▁Yo ▁me ▁pase o ▁por ▁las ▁calle s ▁de ▁la ▁ciudad ▁.\n",
            "▁ “ ▁Pero ▁yo ▁soy\n",
            "▁ 1 0 ▁Tochtl i ▁, ▁ 1 3 4 6 ▁.\n",
            "▁les ▁regala ste ▁cosas ▁muy ▁buenas ▁.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"---Running train_dev_test_split.py---\"\n",
        "!python3 MT-Preparation/train_dev_split/train_dev_test_split.py 1000 1000 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword\n",
        "\n",
        "!echo \"---Counting lines in subword files---\"\n",
        "!wc -l MT-Preparation/*.subword.*\n",
        "\n",
        "!echo \"---First line of each file---\"\n",
        "!head -n 1 MT-Preparation/*.{train,dev,test}\n",
        "\n",
        "!echo \"---Last line of each file---\"\n",
        "!tail -n 1 MT-Preparation/*.{train,dev,test}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTE3MslMWGzE",
        "outputId": "e0e6286b-9d96-4f5d-c79b-1ae0f09f8b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Running train_dev_test_split.py---\n",
            "Dataframe shape: (14783, 2)\n",
            "--- Empty Cells Deleted --> Rows: 14783\n",
            "--- Wrote Files\n",
            "Done!\n",
            "Output files\n",
            "MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.train\n",
            "MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.train\n",
            "MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.dev\n",
            "MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.dev\n",
            "MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test\n",
            "MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test\n",
            "---Counting lines in subword files---\n",
            "   1000 MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.dev\n",
            "   1000 MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test\n",
            "  12783 MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.train\n",
            "   1000 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.dev\n",
            "   1000 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test\n",
            "  12783 MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.train\n",
            "  29566 total\n",
            "---First line of each file---\n",
            "==> MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.train <==\n",
            "▁Y ten ▁de claro ▁gue ▁teng o ▁ve ynte ▁y ▁dos ▁xica ras ▁casi ▁nuevas ▁mando ▁las ▁aya ▁la ▁dicha ▁mi ▁nieta ▁Juan a ▁Maximilian a ▁y ▁Dieg o ▁de ▁Cas tañed a ▁y ▁su ▁hermano ▁[ ▁menor ▁] ▁Balta sar ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.train <==\n",
            "▁Y oan ▁ni cteneua ▁cempoal tetl ▁omome ▁xical pe chtotonti n ▁achi ▁ya ia ncuic ▁yntech ▁pouiz ▁yn ▁noxhui uan ▁Joan a ▁Maximilian a ▁y oan ▁Diego ▁de ▁C astañ eda ▁y oan ▁yn ▁ite icauh ▁Baltha zar ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.dev <==\n",
            "▁E ▁yo ▁el ▁alguacil ▁mayor ▁que ▁fui ▁presente ▁al ▁da lle ▁a ▁los ▁susodicho s ▁la ▁posesion ▁y ▁se ▁pusier on ▁e ▁hinca ron ▁qu atro ▁estaca s ▁por ▁señal ▁de ▁que ▁son ▁de ▁los ▁dichos ▁comprador es ▁las ▁dicha s ▁tierra s ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.dev <==\n",
            "▁Au h ▁i ▁ne uatl ▁alguacil ▁mayor ▁ynic ▁o niquimaxcati ▁y ▁nican ▁no motocayoti que ▁onic quequetz ▁estaca ▁nauhcan ▁ynic ▁machiyotica ▁yn ▁intlal ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test <==\n",
            "▁Lo ▁enredan ▁, ▁lo ▁dispone n ▁, ▁le ▁colocan ▁todo ▁aquello ▁con ▁que ▁lo ▁han ▁de ▁quemar ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test <==\n",
            "▁Quitla ololtia ▁qui tlatepeualti a ▁quitlamamaca ▁in ▁ic ▁mochi ▁iuan ▁quitlatiz que ▁.\n",
            "---Last line of each file---\n",
            "==> MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.train <==\n",
            "▁Le ▁ respondió ▁Temizt zin ▁: ▁Nada ▁; ▁sol amente ▁he ▁venido ▁a ▁saludarlo ▁, ▁sol amente ▁he ▁venido ▁a ▁ofrenda r ▁a ▁mi ▁dios ▁y ▁señor ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.train <==\n",
            "▁Yc ▁quinanquilli ▁quito ▁yn ▁Temizt zin ▁: ▁Ca ▁ hamo ▁; ▁ca ▁çan ▁nicnotlapal huico ▁, ▁ca ▁çan ▁ nicnotlatlauhtil lico ▁yn ▁no teouh ▁yn ▁notlahtoca uh ▁.\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.dev <==\n",
            "▁Que ▁venga ▁de ▁alla\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.dev <==\n",
            "▁Mah ▁hualehua ▁desd e ▁ne\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test <==\n",
            "▁Se pan ▁todos ▁los ▁que ▁leyer en ▁y ▁b ieren ▁como ▁yo ▁Ana ▁Justi na ▁natural ▁de ▁esta ▁ciudad ▁del ▁barrio ▁[ ▁tlaxilacall i ▁] ▁de ▁San ▁Ip olito ▁Teocal titlan ▁con ▁todo ▁my ▁poder ▁y ▁volunta d ▁y ▁en te dim y ento ▁hag o ▁esta ▁escritura ▁con ▁todo ▁lo ▁que ▁en ▁ella ▁declara ▁. ▁[ ▁Marg en ▁izquierd o ▁: ▁venta ▁real ▁que ▁hizo ▁Juan a ▁Justi na ▁y ndia ▁a ▁Luis ▁de ▁Za vall os ▁de ▁una ▁casa ▁en ▁ 3 0 ▁peso s ▁. ▁]\n",
            "\n",
            "==> MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test <==\n",
            "▁Ma ▁quimatican ▁yn ▁ixquichtin ▁quittaz que ▁ynin ▁amatl ▁yn ▁quenin ▁nehuatl ▁Ana n ▁Justi na ▁ni chane ▁yn ▁ipan ▁altepetl ▁ciu dad ▁Mexico ▁notlaxil caltian ▁Sanct ▁Ipal ito ▁Teocalti tlan ▁yca ▁in ▁no huelitiliz ▁in ▁nociyeliz ▁yhuan ▁ notlahuel machiliz ▁ynic ▁huel ▁nicchihua ▁yn ▁escritur a ▁yn ▁ixquich ▁oncan ▁teneuhtica ▁.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_DATA_NAME = \"MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword\"\n",
        "TARGET_DATA_NAME = \"MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword\"\n",
        "\n",
        "config = f\"\"\"\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# Rutas de archivos de entrenamiento\n",
        "#(previamente aplicado subword tokenization)\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: {SRC_DATA_NAME}.train\n",
        "        path_tgt: {TARGET_DATA_NAME}.train\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: {SRC_DATA_NAME}.dev\n",
        "        path_tgt: {TARGET_DATA_NAME}.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabularios (serán generados por `onmt_build_vocab`)\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# Tamaño del vocabulario\n",
        "#(debe concordar con el parametro usado en el algoritmo de subword tokenization)\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filtrado sentencias de longitud mayor a n\n",
        "# actuara si [filtertoolong] está presente\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenizadores\n",
        "src_subword_model: source.model\n",
        "tgt_subword_model: target.model\n",
        "\n",
        "# Archivos donde se guardaran los logs y los checkpoints de modelos\n",
        "log_file: train.log\n",
        "save_model: models/model.nahes\n",
        "\n",
        "# Condición de paro si no se obtienen mejoras significativas\n",
        "# despues de n validaciones\n",
        "early_stopping: 4\n",
        "\n",
        "# Guarda un checkpoint del modelo cada n steps\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# Mantiene los n ultimos checkpoints\n",
        "keep_checkpoint: 3\n",
        "\n",
        "# Reproductibilidad\n",
        "seed: 3435\n",
        "\n",
        "# Entrena el modelo maximo n steps\n",
        "# Default: 100,000\n",
        "train_steps: 3000\n",
        "\n",
        "# Corre el set de validaciones (*.dev) despues de n steps\n",
        "# Defatul: 10,000\n",
        "valid_steps: 1000\n",
        "\n",
        "warmup_steps: 1000\n",
        "report_every: 100\n",
        "\n",
        "# Numero de GPUs y sus ids\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Configuración del optimizador\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Configuración del Modelo\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "\"\"\"\n",
        "\n",
        "# create config file\n",
        "!echo \"\" > ./config.yaml\n",
        "\n",
        "# write config to file\n",
        "with open(\"./config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)\n",
        "\n",
        "# print config\n",
        "!cat ./config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1cT9Iy3WNbQ",
        "outputId": "a00c47ad-04e2-4a9f-fe33-80960332d63c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "## Where the samples will be written\n",
            "save_data: run\n",
            "\n",
            "# Rutas de archivos de entrenamiento\n",
            "#(previamente aplicado subword tokenization)\n",
            "data:\n",
            "    corpus_1:\n",
            "        path_src: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.train\n",
            "        path_tgt: MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.train\n",
            "        transforms: [filtertoolong]\n",
            "    valid:\n",
            "        path_src: MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.dev\n",
            "        path_tgt: MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.dev\n",
            "        transforms: [filtertoolong]\n",
            "\n",
            "# Vocabularios (serán generados por `onmt_build_vocab`)\n",
            "src_vocab: run/source.vocab\n",
            "tgt_vocab: run/target.vocab\n",
            "\n",
            "# Tamaño del vocabulario\n",
            "#(debe concordar con el parametro usado en el algoritmo de subword tokenization)\n",
            "src_vocab_size: 50000\n",
            "tgt_vocab_size: 50000\n",
            "\n",
            "# Filtrado sentencias de longitud mayor a n\n",
            "# actuara si [filtertoolong] está presente\n",
            "src_seq_length: 150\n",
            "src_seq_length: 150\n",
            "\n",
            "# Tokenizadores\n",
            "src_subword_model: source.model\n",
            "tgt_subword_model: target.model\n",
            "\n",
            "# Archivos donde se guardaran los logs y los checkpoints de modelos\n",
            "log_file: train.log\n",
            "save_model: models/model.nahes\n",
            "\n",
            "# Condición de paro si no se obtienen mejoras significativas\n",
            "# despues de n validaciones\n",
            "early_stopping: 4\n",
            "\n",
            "# Guarda un checkpoint del modelo cada n steps\n",
            "save_checkpoint_steps: 1000\n",
            "\n",
            "# Mantiene los n ultimos checkpoints\n",
            "keep_checkpoint: 3\n",
            "\n",
            "# Reproductibilidad\n",
            "seed: 3435\n",
            "\n",
            "# Entrena el modelo maximo n steps\n",
            "# Default: 100,000\n",
            "train_steps: 3000\n",
            "\n",
            "# Corre el set de validaciones (*.dev) despues de n steps\n",
            "# Defatul: 10,000\n",
            "valid_steps: 1000\n",
            "\n",
            "warmup_steps: 1000\n",
            "report_every: 100\n",
            "\n",
            "# Numero de GPUs y sus ids\n",
            "world_size: 1\n",
            "gpu_ranks: [0]\n",
            "\n",
            "# Batching\n",
            "bucket_size: 262144\n",
            "num_workers: 0\n",
            "batch_type: \"tokens\"\n",
            "batch_size: 4096\n",
            "valid_batch_size: 2048\n",
            "max_generator_batches: 2\n",
            "accum_count: [4]\n",
            "accum_steps: [0]\n",
            "\n",
            "# Configuración del optimizador\n",
            "model_dtype: \"fp16\"\n",
            "optim: \"adam\"\n",
            "learning_rate: 2\n",
            "# warmup_steps: 8000\n",
            "decay_method: \"noam\"\n",
            "adam_beta2: 0.998\n",
            "max_grad_norm: 0\n",
            "label_smoothing: 0.1\n",
            "param_init: 0\n",
            "param_init_glorot: true\n",
            "normalization: \"tokens\"\n",
            "\n",
            "# Configuración del Modelo\n",
            "encoder_type: transformer\n",
            "decoder_type: transformer\n",
            "position_encoding: true\n",
            "enc_layers: 6\n",
            "dec_layers: 6\n",
            "heads: 8\n",
            "hidden_size: 512\n",
            "word_vec_size: 512\n",
            "transformer_ff: 2048\n",
            "dropout_steps: [0]\n",
            "dropout: [0.1]\n",
            "attention_dropout: [0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Vocabulary\n",
        "\"\"\"\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %%time\n",
        "# !onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JCVHs0ynWTm9",
        "outputId": "a07462fc-9f61-442c-a3b0-3af8a3c3e0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nVocabulary\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Validate GPU support\n",
        "\"\"\"\n",
        "\n",
        "# Check if the GPU is active\n",
        "!nvidia-smi -L\n",
        "\n",
        "# Check if the GPU is visible to PyTorch\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available.\")\n",
        "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "    print(\"Total GPU memory:\", gpu_memory / 1024**2, \"MB\")\n",
        "else:\n",
        "    print(\"No GPU found. Please check that you have an NVIDIA GPU and installed the appropriate drivers.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZwo3PuXWWwy",
        "outputId": "a3869d5e-72c7-42c7-febb-4e2f898b6014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "No GPU found. Please check that you have an NVIDIA GPU and installed the appropriate drivers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0ifB1LJwXLrO",
        "outputId": "9a527629-7a4c-4737-e572-5291306f9bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTraining\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List the models in the models directory\n",
        "!echo \"---Listing models---\"\n",
        "!ls models\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %%time\n",
        "# !onmt_translate -model models/model.nahes_step_3000.pt -src MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test -output MT-Preparation/americasnlp2021.es.practice.translated -gpu 0 -min_length 1\n",
        "\n",
        "# Display the last few lines of the test dataset\n",
        "!echo \"---Last few lines of the test dataset---\"\n",
        "!tail MT-Preparation/americasnlp2021.nah-es.nah-filtered.nah.subword.test\n",
        "\n",
        "# Display the last few lines of the translated dataset\n",
        "!echo \"---Last few lines of the translated dataset---\"\n",
        "!tail MT-Preparation/americasnlp2021.es.practice.translated\n",
        "\n",
        "# Run the 3-desubword.py script to remove subwording from the test dataset\n",
        "!echo \"---Running 3-desubword.py script on test dataset---\"\n",
        "!python MT-Preparation/subwording/3-desubword.py source.model MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test\n",
        "\n",
        "# Run the 3-desubword.py script to remove subwording from the translated dataset\n",
        "!echo \"---Running 3-desubword.py script on translated dataset---\"\n",
        "!python MT-Preparation/subwording/3-desubword.py target.model MT-Preparation/americasnlp2021.es.practice.translated\n",
        "\n",
        "# Display the last few lines of the desubworded translated dataset\n",
        "!echo \"---Last few lines of the desubworded translated dataset---\"\n",
        "!tail MT-Preparation/americasnlp2021.es.practice.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxdIpAsqXQsR",
        "outputId": "958f6862-d2a6-40f6-ed38-ad96ea44891c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Listing models---\n",
            "ls: cannot access 'models': No such file or directory\n",
            "---Last few lines of the test dataset---\n",
            "▁Ce ▁tecont ontli ▁ihuan ▁tlahco ▁p itzochiyahuizotl\n",
            "▁Ye huiptla ▁.\n",
            "▁Au h ▁yn ▁oya ▁C uxcux tli ▁o manalo que ▁y ▁xochmilca ▁, ▁uel ▁tla nepantla ▁yn ▁quitlali que ▁; ▁nima ▁ye ▁yc ▁ualtemo ▁tetz tzo ualli ▁, ▁nima ▁ye ▁yc ▁ualtemo ▁xiuh couatl ▁, ▁nima ▁ye ▁yc ▁tetl eca uillo ▁, ▁nima ▁ye ▁yc ▁miqui ▁y ▁xochmilca ▁.\n",
            "▁Au h ▁yhuan ▁ye ▁matlacpohual xihuitl ▁ypan ▁yepohual xihuitl ▁ypan ▁matlactlomey ▁xihuitl ▁yn ▁ipan ▁XIII ▁Tecpa tl ▁xihuitl ▁, ▁ 1 3 3 6 ▁años ▁, ▁yn ▁momiquilli co ▁yn ▁Huehue ▁Acamapicht li ▁tlahtohuani ▁Culhua can ▁, ▁yn ▁tlahtocat ▁matlactlomey ▁xiuitl ▁.\n",
            "▁in ▁a ocac ▁huel ▁cempoa ▁,\n",
            "▁Yntlatol ▁conmononochilli que ▁conito que ▁ticcaquilti co ▁yn ▁justicia ▁ipanpa ▁omomiquilli ▁in ▁yehuatl ▁Ysabe l ▁Ana n ▁. ▁Au h ▁tech nahuatitia ▁inic ▁ticnamaca zque ▁yn ▁ical ▁yn ▁itlal ▁ypan ▁quitlalitia ▁yn ▁itestamento ▁ynic ▁alvaceas ▁techcauh tia ▁yn ▁ipatiuh ▁mochiuaz ▁calli ▁tlalli ▁misa ▁y quipan ▁mitoz ▁y palehuilo ca ▁mochihuaz ▁yn ▁ianiman ▁yhuan ▁cequi ▁yc ▁moztla huaz ▁yn ▁qui tepieli aya ▁tomine s ▁.\n",
            "▁Mah ▁amo ▁quimatoca ▁,\n",
            "▁Au h ▁yn ▁omoteneuhtzino ▁d oña ▁Isabel ▁ca ▁ye ▁quin ▁imoztla yoc ▁ipan ▁mi érc oles ▁teotlac ▁macuilli ▁tzilli ▁yn ▁ic ▁ 2 0 ▁mani ▁metztli ▁mayo ▁yn ▁otoco c ▁ynacayo tzin ▁oncan ▁monasterio ▁Con sepció ▁, ▁ymixpan tzinco ▁yn ▁ihuayol que ▁y ▁ye ▁españoles ▁omocuepque ▁quintocayotia ▁Andr ada s ▁.\n",
            "▁Rus otlahtocayo pan ▁motlahtocatlali ▁teuton cihuapill i ▁in ▁itoca ▁Inic ▁Ome ▁Cat alina ▁.\n",
            "▁Ma ▁quimatican ▁yn ▁ixquichtin ▁quittaz que ▁ynin ▁amatl ▁yn ▁quenin ▁nehuatl ▁Ana n ▁Justi na ▁ni chane ▁yn ▁ipan ▁altepetl ▁ciu dad ▁Mexico ▁notlaxil caltian ▁Sanct ▁Ipal ito ▁Teocalti tlan ▁yca ▁in ▁no huelitiliz ▁in ▁nociyeliz ▁yhuan ▁ notlahuel machiliz ▁ynic ▁huel ▁nicchihua ▁yn ▁escritur a ▁yn ▁ixquich ▁oncan ▁teneuhtica ▁.\n",
            "---Last few lines of the translated dataset---\n",
            "tail: cannot open 'MT-Preparation/americasnlp2021.es.practice.translated' for reading: No such file or directory\n",
            "---Running 3-desubword.py script on test dataset---\n",
            "Done desubwording! Output: MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test.desubword\n",
            "---Running 3-desubword.py script on translated dataset---\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MT-Preparation/subwording/3-desubword.py\", line 21, in <module>\n",
            "    with open(target_pred) as pred, open(target_decodeded, \"w+\") as pred_decoded:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'MT-Preparation/americasnlp2021.es.practice.translated'\n",
            "---Last few lines of the desubworded translated dataset---\n",
            "tail: cannot open 'MT-Preparation/americasnlp2021.es.practice.translated.desubword' for reading: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Evaluation\n",
        "\"\"\"\n",
        "\n",
        "!echo \"---Cloning MT-Evaluation repository---\"\n",
        "!git clone https://github.com/ymoslem/MT-Evaluation.git\n",
        "\n",
        "!echo \"---Installing requirements for MT-Evaluation---\"\n",
        "!pip install -r MT-Evaluation/requirements.txt\n",
        "\n",
        "!echo \"---Running 3-desubword.py script---\"\n",
        "!python MT-Preparation/subwording/3-desubword.py target.model MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test\n",
        "\n",
        "!echo \"---Running compute-bleu.py script---\"\n",
        "!python MT-Evaluation/BLEU/compute-bleu.py MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test.desubword MT-Preparation/americasnlp2021.es.practice.translated.desubword"
      ],
      "metadata": {
        "id": "kbFhKvKlXbDI",
        "outputId": "f17a1ce3-fef4-4737-ec32-7218c9688960",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Cloning MT-Evaluation repository---\n",
            "Cloning into 'MT-Evaluation'...\n",
            "remote: Enumerating objects: 101, done.\u001b[K\n",
            "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "Receiving objects: 100% (101/101), 20.17 KiB | 10.08 MiB/s, done.\n",
            "remote: Total 101 (delta 46), reused 4 (delta 1), pack-reused 0\u001b[K\n",
            "Resolving deltas: 100% (46/46), done.\n",
            "---Installing requirements for MT-Evaluation---\n",
            "Collecting jiwer (from -r MT-Evaluation/requirements.txt (line 1))\n",
            "  Downloading jiwer-3.0.4-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r MT-Evaluation/requirements.txt (line 2)) (3.8.1)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from -r MT-Evaluation/requirements.txt (line 3)) (2.4.2)\n",
            "Collecting sacremoses (from -r MT-Evaluation/requirements.txt (line 4))\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer->-r MT-Evaluation/requirements.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer->-r MT-Evaluation/requirements.txt (line 1)) (3.9.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r MT-Evaluation/requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r MT-Evaluation/requirements.txt (line 2)) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->-r MT-Evaluation/requirements.txt (line 2)) (4.66.4)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3)) (1.25.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3)) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->-r MT-Evaluation/requirements.txt (line 3)) (4.9.4)\n",
            "Installing collected packages: sacremoses, jiwer\n",
            "Successfully installed jiwer-3.0.4 sacremoses-0.1.1\n",
            "---Running 3-desubword.py script---\n",
            "Done desubwording! Output: MT-Preparation/americasnlp2021.es-nah.es-filtered.es.subword.test.desubword\n",
            "---Running compute-bleu.py script---\n",
            "Reference 1st sentence: Lo enredan , lo disponen , le colocan todo aquello con que lo han de quemar .\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MT-Evaluation/BLEU/compute-bleu.py\", line 28, in <module>\n",
            "    with open(target_pred) as pred:  \n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'MT-Preparation/americasnlp2021.es.practice.translated.desubword'\n"
          ]
        }
      ]
    }
  ]
}